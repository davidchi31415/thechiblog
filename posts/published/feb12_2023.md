---
title: 'The Variational Autoencoder (VAE)'
date: '2/12/2023'
description: 'Combining basic neural networks with probability distributions and Bayes rule, we get a powerful SOTA model that can be used for generative modeling of, well, just about anything.'
genre: 'ML'
tags: 'Tutorial Notes'
---

Hello. In this blog, I will discuss a particularly interesting type of neural network model: the variational autoencoder. This is a model that was originally introduced in the paper ["Auto-Encoding Variational Bayes"](https://doi.org/10.48550/arXiv.1312.6114) by Kingma et. al back in 2013; it has since become a widely used state-of-the-art generative model. Here I will explain an overview of **what it is**, **how it works**, **how it compares to other models**, and also **how to implement it** in PyTorch.

Note: *I have been using the model and its variants for the past few months, so please view this blog not as a textbook/professionally written piece of text but as an overview of the topic from the perspective of someone still learning it.*

## What is a Variational Autoencoder?

The **Variational Autoencoder**, or **VAE**, is a type of generative model. That is, once it has been trained from a set of examples, it can be used to generate novel examples that (ideally) should be belong to the generative distribution of the data we are trying to model, and thus be similar to the training data. This can serve many tasks, such as image generation in computer vision, text generation in NLP, etc. 

Note: *VAEs are not the only type of such model: GANs and more recently, Denoising Diffusion models, are other examples. We will see more on that later, particularly how VAEs perform better/worse.*

## How does the VAE work?

### Part 1: Architecture

To understand a VAE, let's first talk about its cousin, the **Autoencoder** (or just **AE**). An autoencoder is a model that attempts to compress data and then, drawing from a lower-dimensional space, reconstruct the original data as well as possible. This is achieved through the following:
* We have an **encoder**, which maps an input training example to a lower-dimensional space, typically through a neural network. 
* We call such encodings the **latent space**, because they represent our data in a compressed space and are thus required to retain only the most vital details about the input. These latent variables are vectors that encode (probably nonlinear) combinations of dimensions from the original data into each dimension of the latent space.
* The latent variables are then fed through a **decoder**, which maps the low-dimensional vectors into a higher-dimensional space. The output of the decoder is meant to be a reconstruction of the input.

![A Hand-drawn Cartoon Depiction of an Autoencoder's Architecture](/AE.png)

Now, let's see the difference between an AE and a VAE. Consider the following situation.

Imagine we have some image of a brown cat, and we'd like another (different) image of a similar-looking brown cat. Also imagine that we have a large dataset of images of cats and dogs. If there was some latent space in which each image from our dataset was mapped to its own continuous distribution (i.e., each brown cat, orange cat, brown dog, etc.), we could simply sample from the distribution of our original image to obtain another one that satisfies the desired properties. We could also get mixes of features if we interpolate through the different distributions. 

In the case of an AE, our latent space consists of explicit points; that is, each example image is mapped to a point and not a distribution. If one were to attempt to sample by picking random points around such a latent mapping, the result may not be what we expected. This is because the model only learns to compress the data (much like PCA), which may be useful for featurization but not for generative modeling.

This is where the variational part of a VAE comes in. A VAE consists of the same components as the AE, except for the following:
* Our **encoder** in a VAE maps each of its inputs to a mean $\mu$ and variance $\sigma^{2}$ instead of a discrete point. These parameters define what we call a posterior distribution, $q(z|x)$. If you are familiar with probability/statistics, the idea of a posterior distribution is that it is the probability of an event conditioned on another (i.e., "what's the probability of latent variable $z$ given an input $x$?"). In practice, it is most common to model a normal distribution (although technically other distributions would work as well) for the sake of easier backpropagation and simplicity.
* The parameters from the encoder are then used for sampling. We draw some vector $z=\mu + \sigma*\mathcal{N}(0, 1)$ and feed this into our decoder, which behaves as normal.
As you can see, the VAE allows each example to be mapped to a continuous normal distribution. Whereas the AE did not have a continuous latent space, our VAE does, making sampling/generation possible.

![A Hand-drawn Cartoon Depiction of a Variational Autoencoder's Architecture](/VAE.png)

### Part 2: Training

Now that we've established what the VAE looks like and how it differs from an AE, let's consider how to train the model.

At a high level, we have the following loss function: 

$$
% \begin{align}
    \large{
        \color{black}{
            \mathcal{L} = \mathcal{L}_{\text{reconstruction}} + \text{KL-Divergence}
        }
    }
% \end{align}
$$

The left term, $\mathcal{L}_{\text{reconstruction}}$ is relatively straightforward: if we want our model to really work, it needs to be able to reconstruct any given input with reasonable accuracy. Otherwise, it isn't really learning the important latent details about the data.

The second term, the KL-Divergence, needs some more explanation.

### Part 3: Generation

## VAE vs. GAN vs DDPM

## Implementation
